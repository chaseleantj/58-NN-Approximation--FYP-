# Approximation Properties of Neural Networks

Neural networks (NN) can have different activation functions. By far the most common activation function is the rectified linear unit (ReLU).

However, we can consider other more "exotic" activation functions like the cosine.

Furthermore, theoretical results in Barron (1993) and Xu (2020) show that some target functions can be more naturally expressed in terms of cosine functions.

Therefore, we run some tests and see that in some circumstances, networks with cosine activation can outperform networks with ReLU activation.

## Table of contents
* [Regression](#demo)
* [Classification](#general-info)
* [The MNIST dataset](#the-mnist-dataset)
* [Run the code](#run-the-code)
* [References](#references)

## Regression

## Classification

## The MNIST dataset

## Run the code

## References

The code inside the directory `lib` is adapted from the book Neural Networks from Scratch in Python.

The MNIST dataset in the directory `data` was downloaded from the following source:
