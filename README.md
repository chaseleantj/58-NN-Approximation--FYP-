# Approximation Properties of Neural Networks

Neural nets can have different activation functions. By far the most common activation function is the rectified linear unit (ReLU).

However, many other interesting activation functions such as the cosine function can be considered. Furthermore, theoretical results in Barron (1993) and Xu (2020) show that some target functions can be more naturally expressed in terms of cosine functions. Therefore, we run some experiments to test that in some circumstances, networks with cosine activation can outperform networks with ReLU activation. 
