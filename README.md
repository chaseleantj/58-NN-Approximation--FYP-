# Approximation Properties of Neural Networks

Neural networks (NN) can have different activation functions. By far the most common activation function is the rectified linear unit (ReLU).

However, we can consider other more "exotic" activation functions like the cosine.

Furthermore, theoretical results in Barron (1993) and Xu (2020) show that some target functions can be more naturally expressed in terms of cosine functions.

Therefore, we run some tests on single layer NNs and see that in some circumstances, networks with cosine activation can outperform networks with ReLU activation.

## Table of contents
* [Regression](#demo)
* [Classification](#general-info)
* [The MNIST dataset](#the-mnist-dataset)
* [Run the code](#run-the-code)
* [References](#references)

## Regression

## Classification

## The MNIST dataset

## Run the code

Clone the repository and run `main.py`.

For a more in-depth look at how each component of the neural network functions, run a simplified version inside `demo.py`. The output is identical.

## References

The code inside the directory `lib` is adapted from the book Neural Networks from Scratch in Python.

The MNIST dataset in the directory `data` was downloaded from the following source:
